# -*- coding: utf-8 -*-
"""Chatboat_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gWTCXhFwAWJvvA4tVHisuRxILjqDhfS2
"""

import random
import json
import pickle

import nltk
from nltk.stem import WordNetLemmatizer
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, Dropout
from tensorflow.keras.optimizers import SGD

import numpy as np

{
    "tag" : "Name of the Disease",
    "patterns" : ["comma separated symptoms"],
    "responses" : ["Answer user will receive i.e the disease user might have"]
  }

{"intents": [
  {
    "tag" : "Name of the Disease",
    "patterns" : ["comma separated symptoms"],
    "responses" : ["Answer user will receive i.e the disease user might have"]
  },
  {
    "tag" : "Name of the Disease 2",
    "patterns" : ["comma separated symptoms"],
    "responses" : ["Answer user will receive i.e the disease user might have"]
  },

  ]
}

nltk.download('punkt')
lemmatizer = WordNetLemmatizer()

import os
print("Current working directory:", os.getcwd())

from google.colab import files
uploaded = files.upload()

intents = json.loads(open("intents.json").read())

words = []
classes = []
documents = []

ignore_letters = ["?", "!", ".", ","]

import nltk

# Download the full punkt package
nltk.download('punkt')

import nltk.data
print(nltk.data.path)

nltk.data.path.append('/root/nltk_data')

import nltk
import shutil
import os

# Remove existing nltk_data directory to clear corrupted files
nltk_data_dir = '/root/nltk_data'
if os.path.exists(nltk_data_dir):
    shutil.rmtree(nltk_data_dir)

# Re-download 'punkt'
nltk.download('punkt')

import nltk

nltk.download('punkt_tab')

import nltk

# Download the 'punkt' tokenizer
nltk.download('punkt')

# Sample text to tokenize
sample_text = "Hello! How are you doing today?"
tokens = nltk.word_tokenize(sample_text)

# Print the tokens
print(tokens)

for intent in intents["intents"]:
    for pattern in intent["patterns"]:
        word_list = nltk.word_tokenize(pattern)
        words.extend(word_list)
        documents.append((word_list, intent["tag"]))

        if intent["tag"] not in classes:
            classes.append(intent["tag"])

import nltk
nltk.download('wordnet')
words = [lemmatizer.lemmatize(word)
         for word in words if word not in ignore_letters]

words = sorted(set(words))
classes = sorted(set(classes))

import pickle
pickle.dump(words, open('words.pkl', 'wb'))
pickle.dump(classes, open('classes.pkl', 'wb'))

import random
import numpy as np

# Initialize the dataset
dataset = []
template = [0] * len(classes)  # One-hot encoding template

# Process each document and create a bag of words representation
for document in documents:
    bag = []
    word_patterns = document[0]  # Get the tokenized words from the document
    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]  # Lemmatize and lowercase

    # Create a binary bag of words (1 if the word is in the document, 0 otherwise)
    for word in words:
        bag.append(1) if word in word_patterns else bag.append(0)

    # One-hot encode the tag (class)
    output_row = list(template)
    output_row[classes.index(document[1])] = 1

    # Append the bag of words and one-hot encoded class to the dataset
    dataset.append([bag, output_row])

# Shuffle the dataset before converting
random.shuffle(dataset)

# Separate features (X) and labels (y)
train_x = np.array([item[0] for item in dataset])  # Bag of words (features)
train_y = np.array([item[1] for item in dataset])  # One-hot encoded classes (labels)

# Check the shape of train_x and train_y
print(f"train_x shape: {train_x.shape}, train_y shape: {train_y.shape}")

from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.optimizers import SGD

# Initialize the model
model = Sequential()
model.add(Dense(256, input_shape=(len(train_x[0]),), activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(len(train_y[0]), activation='softmax'))

# Define the optimizer
sgd = SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)

# Compile the model
model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])

# Fit the model
hist = model.fit(train_x, train_y, epochs=200, batch_size=5, verbose=1)

# Save the model
model.save("chatbot_model.h5")
print("Model saved successfully!")

!pip install SpeechRecognition

!pip install pyttsx3

from google.colab import drive
drive.mount('/content/drive')

import random
import json
import pickle
import nltk
from nltk.stem import WordNetLemmatizer
from tensorflow.keras.models import load_model
import numpy as np
import time

# Check for missing modules and provide installation instructions
missing_modules = []
try:
    import speech_recognition as sr
except ImportError:
    missing_modules.append("speechrecognition")
try:
    import pyttsx3
except ImportError:
    missing_modules.append("pyttsx3")

if missing_modules:
    print("Error: The following modules are not installed. Please install them using pip:")
    for module in missing_modules:
        if module == "speechrecognition":
            print(f"- pip install SpeechRecognition")
        elif module == "pyttsx3":
             print(f"- pip install pyttsx3")
    exit()  # Exit the program if modules are missing

# Download nltk resources if they are not present
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
try:
    nltk.data.find('corpora/wordnet')
except LookupError:
    nltk.download('wordnet')

lemmatizer = WordNetLemmatizer()
intents = json.loads(open("intents.json").read())  # Use a context manager here

words = pickle.load(open('words.pkl', 'rb'))  # Use a context manager here
classes = pickle.load(open('classes.pkl', 'rb'))  # Use a context manager here
model = load_model('chatbot_model.h5')  # Keras models are safe to load directly


def clean_up_sentence(sentence):
    """Tokenizes and lemmatizes the input sentence."""
    sentence_words = nltk.word_tokenize(sentence)
    sentence_words = [
        lemmatizer.lemmatize(word) for word in sentence_words
    ]
    return sentence_words


def bag_of_words(sentence):
    """
    Creates a bag-of-words representation of the sentence.

    Args:
        sentence: The input sentence (string).

    Returns:
        A numpy array representing the bag-of-words.
    """
    sentence_words = clean_up_sentence(sentence)
    bag = [0] * len(words)
    for i, word in enumerate(words):
        if word in sentence_words:  # Check for presence instead of equality
            bag[i] = 1
    return np.array(bag)


def predict_class(sentence):
    """
    Predicts the class (intent) of the sentence using the trained model.

    Args:
        sentence: The input sentence (string).

    Returns:
        A list of dictionaries, where each dictionary contains the intent
        and its probability.  Returns an empty list if no intents pass
        the threshold.
    """
    bow = bag_of_words(sentence)
    res = model.predict(np.array([bow]))[0]
    ERROR_THRESHOLD = 0.25
    results = [
        (i, r) for i, r in enumerate(res) if r > ERROR_THRESHOLD
    ]  # Use tuple for efficiency
    results.sort(key=lambda x: x[1], reverse=True)
    return_list = [
        {'intent': classes[i], 'probability': str(r)} for i, r in results
    ]
    return return_list



def get_response(intents_list, intents_json):
    """
    Retrieves a response from the intents JSON based on the predicted intent.

    Args:
        intents_list: A list of predicted intents (output of predict_class).
        intents_json: The loaded intents JSON data.

    Returns:
        A randomly chosen response string, or an empty string if no matching
        intent is found.
    """
    if not intents_list:
        return ""  # Return empty string if no intents were found
    tag = intents_list[0]['intent']
    list_of_intents = intents_json['intents']
    for i in list_of_intents:
        if i['tag'] == tag:
            return random.choice(i['responses'])
    return ""  # Return empty string if no matching intent is found



def calling_the_bot(txt, engine, intents_json):
    """
    Predicts the intent of the input text, gets a response, and speaks it.

    Args:
        txt: The input text (string).
        engine: The pyttsx3 engine object.
        intents_json: the intents json
    """
    predict = predict_class(txt)
    res = get_response(predict, intents_json)  # Pass intents_json
    if res:
        engine.say("Found it. From our Database we found that " + res)
        engine.runAndWait()
        print("Your Symptom was  : ", txt)
        print("Result found in our Database : ", res)
    else:
        engine.say(
            "Sorry, I could not find information about that symptom in our database."
        )
        engine.runAndWait()
        print(
            "Sorry, I could not find information about that symptom in our database."
        )



if __name__ == '__main__':
    print("Bot is Running")
    recognizer = sr.Recognizer()
    mic = sr.Microphone()
    engine = pyttsx3.init()
    engine.setProperty('rate', 175)
    engine.setProperty('volume', 1.0)
    voices = engine.getProperty('voices')

    engine.say(
        "Hello user, I am Bagley, your personal Talking Healthcare Chatbot."
    )
    engine.runAndWait()

    engine.say(
        "IF YOU WANT TO CONTINUE WITH MALE VOICE PLEASE SAY MALE. OTHERWISE SAY FEMALE."
    )
    engine.runAndWait()

    # Voice selection
    with mic as source:
        print("Say 'Male' or 'Female' for voice selection...")
        recognizer.adjust_for_ambient_noise(source, duration=0.2)
        try:
            audio = recognizer.listen(source)
            voice_choice = recognizer.recognize_google(audio)
            if voice_choice.lower() == "female":
                engine.setProperty('voice', voices[1].id)
                print("You have chosen to continue with Female Voice")
            else:
                engine.setProperty('voice', voices[0].id)
                print("You have chosen to continue with Male Voice")
        except sr.UnknownValueError:
            print("Could not understand voice choice. Defaulting to Male voice.")
            engine.setProperty('voice', voices[0].id)
        except sr.RequestError as e:
            print(
                f"Could not request results from Google Speech Recognition service; {e}"
            )
            engine.say("Speech recognition service is currently unavailable.")
            engine.runAndWait()
            exit()  # Exit the program if speech recognition fails

    # Main loop
    continue_conversation = True  # changed to boolean
    while continue_conversation:
        with mic as symptom:
            print("Say Your Symptoms. The Bot is Listening")
            engine.say("You may tell me your symptoms now. I am listening")
            engine.runAndWait()
            try:
                recognizer.adjust_for_ambient_noise(symptom, duration=0.2)
                symp = recognizer.listen(symptom)
                text = recognizer.recognize_google(symp)
                engine.say("You said {}".format(text))
                engine.runAndWait()

                engine.say(
                    "Scanning our database for your symptom. Please wait."
                )
                engine.runAndWait()
                time.sleep(1)
                calling_the_bot(text, engine, intents)  # Pass engine and intents_json

            except sr.UnknownValueError:
                engine.say(
                    "Sorry, Either your symptom is unclear to me or it is not present in our database. Please Try Again."
                )
                engine.runAndWait()
                print(
                    "Sorry, Either your symptom is unclear to me or it is not present in our database. Please Try Again."
                )
            except sr.RequestError as e:
                engine.say(
                    "Speech recognition service is currently unavailable. Please try again later."
                )
                engine.runAndWait()
                print(
                    f"Could not request results from Google Speech Recognition service; {e}"
                )
                continue  # Go to the next iteration of the loop
            except Exception as e:  # Catch other exceptions
                print(f"An unexpected error occurred: {e}")
                engine.say("Sorry, an unexpected error occurred. Please try again later.")
                engine.runAndWait()
                continue

        with mic as ans:
            print("Do you want to continue? Say 'Yes' or 'No'.")
            engine.say("Do you want to continue? Please say 'Yes' or 'No'.")
            engine.runAndWait()
            recognizer.adjust_for_ambient_noise(ans, duration=0.2)
            try:
                voice = recognizer.listen(ans)
                final = recognizer.recognize_google(voice)
                if final.lower() in ('no', 'exit'):
                    continue_conversation = False
                    engine.say("Thank You. Shutting Down now.")
                    engine.runAndWait()
                    print("Bot has been stopped by the user")
                elif final.lower() != 'yes':
                    engine.say("I did not understand. Please say 'Yes' or 'No'.")
                    engine.runAndWait()
            except sr.UnknownValueError:
                engine.say("I did not understand. Please say 'Yes' or 'No'.")
                engine.runAndWait()
            except sr.RequestError as e:
                engine.say("Speech recognition service is currently unavailable. Please try again.")
                engine.runAndWait()
                print(
                    f"Could not request results from Google Speech Recognition service: {e}"
                )
                continue  # restart the loop
            except Exception as e:  # catch other exceptions
                print(f"An unexpected error occurred: {e}")
                engine.say("Sorry, an unexpected error occurred. Please try again later.")
                engine.runAndWait()
                continue

    print("Exiting program.")  # Program exit message

